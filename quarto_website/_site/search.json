[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hello, I am Jonathan Ye\nI am an undergraduate senior studying Public Health and Applied Mathematics + Statistics at Homewood. As a child, I enjoyed playing outside and collecting rocks. Here is a picture of me that I use for my Linkedin, and other things.\n{width = 200}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In greater detail, I am a student who enjoys learning new math and coding sometimes. Public health is interesting, I think it is good to think about research in the context of things that matter a great deal to the general population. I think researchers and scientists tend to only focus on their research/science, and neglect other factors that are also important to consider. For example, I hear a lot of people say that policymakers should just do what science tells them to do, but there are several other things to keep in mind. Feasibility, budget, ethics, and much more are also important.\nI am currently applying to graduate schools for PhD and also industry jobs, because I would like to have options of what to do next after undergrad. I am interested primarily in the health sector because I think it could be fulfilling, and am looking for data science jobs because that’s where most of my skills lie. I lack in biology/chemistry knowledge (definitely in terms of coursework) so I am hoping to get hired based on technological skillset."
  },
  {
    "objectID": "example_analysis.html",
    "href": "example_analysis.html",
    "title": "Example Analysis",
    "section": "",
    "text": "Here, I will outline my goals for this analysis, load the data, preprocess it, share the data dictionary, etc.\n\n\nAre there any characteristics that can help us predict whether a given person prefers mountains or beaches? (i.e. could we predict the answer to the question ‘are you a mountain or a beach person’)\n\n\n\nThe intended audience is anyone who feels strongly about mountains or beaches.\n\n\n\nhttps://www.kaggle.com/datasets/jahnavipaliwal/mountains-vs-beaches-preference\n\n# import kagglehub\n\n# # Download latest version\n# path = kagglehub.dataset_download(\"jahnavipaliwal/mountains-vs-beaches-preference\")\n\n# print(\"Path to dataset files:\", path)\n\nI have moved the file to the data folder\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\ndata = read.csv('data/mountains_vs_beaches_preferences.csv')\ncat(' there are', nrow(data), \"participatns, and\", ncol(data) - 1, \"predictors\")\n\n there are 52444 participatns, and 13 predictors\n\n\n\n\n\n\ndata_dictionary = read.csv('data/data_dictionary.csv')\nknitr::kable(data_dictionary)\n\n\n\n\n\n\n\n\n\nvariables\ntype\ndescription\n\n\n\n\nAge\ninteger\nAge of the individual (numerical).\n\n\nGender\ncharacter\nGender identity of the individual (categorical: male, female, non-binary).\n\n\nIncome\ninteger\nAnnual income of the individual (numerical).\n\n\nEducation_Level\ncharacter\nHighest level of education attained (categorical: high school, bachelor, master, doctorate).\n\n\nTravel_Frequency\ninteger\nNumber of vacations taken per year (numerical).\n\n\nPreferred_Activities\ncharacter\nActivities preferred by individuals during vacations (categorical: hiking, swimming, skiing, sunbathing).\n\n\nVacation_Budget\ninteger\nBudget allocated for vacations (numerical).\n\n\nLocation\ncharacter\nType of residence (categorical: urban, suburban, rural).\n\n\nProximity_to_Mountains\ninteger\nDistance from the nearest mountains (numerical, in miles).\n\n\nProximity_to_Beaches\ninteger\nDistance from the nearest beaches (numerical, in miles).\n\n\nFavorite_Season\ncharacter\nPreferred season for vacations (categorical: summer, winter, spring, fall).\n\n\nPets\ninteger\nIndicates whether the individual owns pets (binary: 0 = No, 1 = Yes).\n\n\nEnvironmental_Concerns\ninteger\nIndicates whether the individual has environmental concerns (binary: 0 = No, 1 = Yes).\n\n\nPreference\ninteger\nIndicates whether the individual prefers mountains or beaches (binary: 0 = beaches, 1 = mountains)"
  },
  {
    "objectID": "example_analysis.html#guiding-question",
    "href": "example_analysis.html#guiding-question",
    "title": "Example Analysis",
    "section": "",
    "text": "Are there any characteristics that can help us predict whether a given person prefers mountains or beaches? (i.e. could we predict the answer to the question ‘are you a mountain or a beach person’)"
  },
  {
    "objectID": "example_analysis.html#intended-audience",
    "href": "example_analysis.html#intended-audience",
    "title": "Example Analysis",
    "section": "",
    "text": "The intended audience is anyone who feels strongly about mountains or beaches."
  },
  {
    "objectID": "example_analysis.html#link-to-data",
    "href": "example_analysis.html#link-to-data",
    "title": "Example Analysis",
    "section": "",
    "text": "https://www.kaggle.com/datasets/jahnavipaliwal/mountains-vs-beaches-preference\n\n# import kagglehub\n\n# # Download latest version\n# path = kagglehub.dataset_download(\"jahnavipaliwal/mountains-vs-beaches-preference\")\n\n# print(\"Path to dataset files:\", path)\n\nI have moved the file to the data folder\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\ndata = read.csv('data/mountains_vs_beaches_preferences.csv')\ncat(' there are', nrow(data), \"participatns, and\", ncol(data) - 1, \"predictors\")\n\n there are 52444 participatns, and 13 predictors"
  },
  {
    "objectID": "example_analysis.html#data-wranglingpreprocessing",
    "href": "example_analysis.html#data-wranglingpreprocessing",
    "title": "Example Analysis",
    "section": "Data wrangling/Preprocessing",
    "text": "Data wrangling/Preprocessing\nHere are the assumptions:\n\nBinary Outcome Variable: Logistic regression is used for binary classification, so the dependent variable should be binary (e.g., 0 or 1, True or False).\n\nThis assumption is met by our dataset.\n\nIndependence of Observations: The observations should be independent of each other. No observation should influence another.\n\nThis assumption should be met, based on how the data was collected. Hopefully the participants are from different families.\n\nNo Perfect Multicollinearity: The predictor variables should not be perfectly correlated with each other. High multicollinearity can make it difficult to determine the individual effect of each predictor.\n\nWe will need to check this assumption.\n\nLarge Sample Size: Logistic regression requires a sufficiently large sample size to produce reliable results.\n\nWith a sample size of 5244 and 13 predictors, this assumption is more than met. In fact, we can even consider adding a couple of features from nonlinear combinations of our original 13 predictors, and still be fine.\n\nNo Significant Outliers: Outliers can influence the model, since Logistic regression minimizes a loss function - and outliers introduce too much loss that the model can try to correct for.\n\nWe will need to check this assumption.\n\nObservations Need to be Properly Distributed Across the Levels of the Dependent Variable: There should be a reasonable number of cases in both categories of the dependent variable. Extreme imbalances can lead to a biased model.\n\nWe will need to check this assumption.\nFIND A SOURCE FOR THIS: https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-logistic-regression/\n\nfeatures = dplyr::select(data, -Preference)\nhead(features)\n\n  Age     Gender Income Education_Level Travel_Frequency Preferred_Activities\n1  56       male  71477        bachelor                9               skiing\n2  69       male  88740          master                1             swimming\n3  46     female  46562          master                0               skiing\n4  32 non-binary  99044     high school                6               hiking\n5  60     female 106583     high school                5           sunbathing\n6  25       male 110588       doctorate                3           sunbathing\n  Vacation_Budget Location Proximity_to_Mountains Proximity_to_Beaches\n1            2477    urban                    175                  267\n2            4777 suburban                    228                  190\n3            1469    urban                     71                  280\n4            1482    rural                     31                  255\n5             516 suburban                     23                  151\n6            2895    urban                      6                   47\n  Favorite_Season Pets Environmental_Concerns\n1          summer    0                      1\n2            fall    0                      1\n3          winter    0                      0\n4          summer    1                      0\n5          winter    1                      1\n6            fall    0                      1\n\n\nIn order to use our categorical features, we must convert them to factors. Moreover, some of our categorial features are ordinal, such as the education variable.\n\nfeatures &lt;- features %&gt;%\n    mutate(across(where(is.character), as.factor))\n\neducation_levels &lt;- c(\"high school\", \"bachelor\", \"master\", \"doctorate\")\n\nfeatures &lt;- features %&gt;%\n    mutate(Education_Level = factor(features$Education_Level, levels = education_levels, ordered = TRUE))\n\nstr(features)\n\n'data.frame':   52444 obs. of  13 variables:\n $ Age                   : int  56 69 46 32 60 25 38 56 36 40 ...\n $ Gender                : Factor w/ 3 levels \"female\",\"male\",..: 2 2 1 3 1 2 2 3 2 3 ...\n $ Income                : int  71477 88740 46562 99044 106583 110588 22245 109411 22531 90840 ...\n $ Education_Level       : Ord.factor w/ 4 levels \"high school\"&lt;..: 2 3 3 1 1 4 2 2 3 1 ...\n $ Travel_Frequency      : int  9 1 0 6 5 3 1 8 6 1 ...\n $ Preferred_Activities  : Factor w/ 4 levels \"hiking\",\"skiing\",..: 2 4 2 1 3 3 4 3 4 4 ...\n $ Vacation_Budget       : int  2477 4777 1469 1482 516 2895 4994 3656 2408 4044 ...\n $ Location              : Factor w/ 3 levels \"rural\",\"suburban\",..: 3 2 3 1 2 3 1 3 2 1 ...\n $ Proximity_to_Mountains: int  175 228 71 31 23 6 157 210 218 271 ...\n $ Proximity_to_Beaches  : int  267 190 280 255 151 47 225 166 263 15 ...\n $ Favorite_Season       : Factor w/ 4 levels \"fall\",\"spring\",..: 3 1 4 3 4 1 1 4 3 3 ...\n $ Pets                  : int  0 0 0 1 1 0 0 1 1 1 ...\n $ Environmental_Concerns: int  1 1 0 0 1 1 1 0 1 1 ...\n\n\n\nMulticollinearity\nWe will plot all of the pairwise correlations in a correlation matrix. In our cases, we will remove a variable if there is a correlation of 0.9 or higher, which is essentially perfectly correlated in practice.\n\nnumeric_features &lt;- features %&gt;%\n    mutate(across(where(is.factor), as.numeric))\ncor_matrix &lt;- stats::cor(numeric_features, use = \"complete.obs\")\n\ncorrplot::corrplot(cor_matrix, method = \"color\", tl.cex = 0.7, addCoef.col = \"black\")\n\n\n\n\n\n\n\n\nWe see that none of our variables are perfectly correlated, so this is great!\n\n\nOutlier Analysis\nOutliers only apply to our numerical variables, such as Age, Income, Travel_Frequency, Vacation_Budget, Proximity_to_Mountains, Proximity_to_Beaches. ::: {.column-margin} We will identify values as outliers if they meet the condition:\n\\[|value - median| &gt; 1.5 \\cdot IQR\\]\nwhere IQR is the interquartile range. :::\nFirst, it may help us to just look at the distributions of the numerical variables - I will plot histograms separately because of bin width issues.\n\nnumeric_features = dplyr::select(data, where(is.numeric)) %&gt;% \n    select(where(~ n_distinct(.) &gt; 2))\n\n\nggplot(numeric_features, aes(x = Age)) +\n    geom_histogram(binwidth = 5) + \n    labs(title = \"Histogram of Participant Age Distribution\")\n\n\n\n\n\n\n\nggplot(numeric_features, aes(x = Income)) +\n    geom_histogram(binwidth = 10000) + \n    labs(title = \"Histogram of Participant Income Distribution\")\n\n\n\n\n\n\n\nggplot(numeric_features, aes(x = Travel_Frequency)) +\n    geom_histogram(binwidth = 1) + \n    labs(title = \"Histogram of Participant Travel Frequency Distribution\")\n\n\n\n\n\n\n\nggplot(numeric_features, aes(x = Vacation_Budget)) +\n    geom_histogram(binwidth = 250) + \n    labs(title = \"Histogram of Participant Vacation Budget Distribution\")\n\n\n\n\n\n\n\nggplot(numeric_features, aes(x = Proximity_to_Mountains)) +\n    geom_histogram(binwidth = 50) + \n    labs(title = \"Histogram of Participant Proximity to Mountains Distribution\")\n\n\n\n\n\n\n\nggplot(numeric_features, aes(x = Proximity_to_Beaches)) +\n    geom_histogram(binwidth = 50) + \n    labs(title = \"Histogram of Participant Proximity to Beaches Distribution\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOk…these values are very odd. I am going to assume that this data set is either already preprocessed, or it’s entirely synthetic data.\n\n\n\nidentify_outliers &lt;- function(x) {\n  IQR &lt;- IQR(x, na.rm = TRUE)\n  median &lt;- median(x, na.rm = TRUE)\n  lower_bound &lt;- median - 1.5 * IQR\n  upper_bound &lt;- median + 1.5 * IQR\n  return(x &lt; lower_bound | x &gt; upper_bound)\n}\n\noutliers &lt;- numeric_features %&gt;%\n  mutate(across(everything(), identify_outliers))\n\nprint(colSums(outliers))\n\n                   Age                 Income       Travel_Frequency \n                     0                      0                      0 \n       Vacation_Budget Proximity_to_Mountains   Proximity_to_Beaches \n                     0                      0                      0 \n\n\nman this dataset actually sucks (too good, no preprocessing for me to do) there are zero outliers. This is 99% synthetic data, and for whatever reason it was generated from uniform distributions, or something like that.\n\n\nObservation distributions\nWe need to ensure that we have an approximately equal number of mountain and beach enjoyers. Something in the neighborhood of 50-50 would be preferred. A pie chart should do the trick.\n\npie_data &lt;- data.frame(\n    group=c('mountains', 'beaches'),\n    value=c(sum(data$Preference == '1'), sum(data$Preference == '0'))\n    )\n\nggplot(pie_data, aes(x=\"\", y=value, fill=group)) +\n    geom_bar(stat=\"identity\", width=1) +\n    coord_polar(\"y\", start=0)\n\n\n\n\n\n\n\ncat('number of mountain enjoyers:', sum(data$Preference == '1'), '\\nnumber of beach enjoyers:', sum(data$Preference == '0'), '\\n')\n\nnumber of mountain enjoyers: 13148 \nnumber of beach enjoyers: 39296 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nAHA! Ok, that’s not good, we can’t have this imbalanced of classes. I’m just going to randomly sample 13148 beach enjoyers.\n\n\n\nnum_mountain_enjoyers &lt;- sum(data$Preference == '1')\nbeach_sample &lt;- data %&gt;%\n  filter(Preference == '0') %&gt;%\n  sample_n(num_mountain_enjoyers)\n\nbalanced_data &lt;- data %&gt;%\n  filter(Preference == '1') %&gt;%\n  bind_rows(beach_sample)\n\nnum_mountain_enjoyers &lt;- sum(balanced_data$Preference == '1')\nnum_beach_enjoyers &lt;- sum(balanced_data$Preference == '0')\ncat('Number of mountain enjoyers:', num_mountain_enjoyers, '\\nNumber of beach enjoyers:', num_beach_enjoyers, '\\n')\n\nNumber of mountain enjoyers: 13148 \nNumber of beach enjoyers: 13148 \n\npie_data &lt;- data.frame(\n  group = c('mountains', 'beaches'),\n  value = c(num_mountain_enjoyers, num_beach_enjoyers)\n)\n\nggplot(pie_data, aes(x = \"\", y = value, fill = group)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\", start = 0) +\n  labs(title = \"Balanced Preference Distribution\")\n\n\n\n\n\n\n\nprint(pie_data)\n\n      group value\n1 mountains 13148\n2   beaches 13148\n\n\n\n\nFeature Engineering\nNormalizing the data is generally a good thing to do. Not super necessary here, but I will do it anyways. https://www.turing.com/kb/effects-of-normalization-techniques-on-logistic-regression-in-data-science\n\nnormalized_data &lt;- balanced_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(where(~ n_distinct(.) &gt; 2)) %&gt;%\n  mutate(across(everything(), ~ (.-mean(., na.rm = TRUE))/sd(., na.rm = TRUE)))\n\nprint(sd(normalized_data$Income))\n\n[1] 1\n\ncategorical_data &lt;- balanced_data %&gt;% select(!where(is.numeric) | where(~ n_distinct(.) == 2)) \n\nnormalized_data &lt;- bind_cols(categorical_data, normalized_data) \nhead(normalized_data)\n\n      Gender Education_Level Preferred_Activities Location Favorite_Season Pets\n1       male        bachelor               skiing    urban          summer    0\n2     female          master               skiing    urban          winter    0\n3 non-binary     high school               hiking    rural          summer    1\n4     female        bachelor               hiking suburban          summer    1\n5       male     high school               hiking suburban            fall    0\n6     female          master               skiing    urban            fall    1\n  Environmental_Concerns Preference        Age      Income Travel_Frequency\n1                      1          1  0.8339919  0.05253978        1.5714037\n2                      0          1  0.1686888 -0.81155439       -1.5613962\n3                      0          1 -0.7627354  1.00860978        0.5271370\n4                      1          1 -0.1639627 -0.59541813        0.8752259\n5                      0          1  0.9005222 -0.14427878       -0.5171296\n6                      1          1 -0.1639627  1.03517595       -0.8652185\n  Vacation_Budget Proximity_to_Mountains Proximity_to_Beaches\n1     -0.20240528              0.4873456            1.1860580\n2     -0.97893258             -0.7365135            1.3397235\n3     -0.96891784             -1.2072285            1.0442129\n4     -0.88494813             -0.0422088            1.0323925\n5     -0.10225790             -1.1483891            1.0796742\n6      0.06953335             -0.6659062            0.5004734\n\n\nNow, need to convert this subset of the data into factors where appropriate\n\nnormalized_data &lt;- normalized_data %&gt;%\n    mutate(across(where(is.character), as.factor))\n\neducation_levels &lt;- c(\"high school\", \"bachelor\", \"master\", \"doctorate\")\n\nnormalized_data &lt;- normalized_data %&gt;%\n    mutate(Education_Level = factor(Education_Level, levels = education_levels, ordered = TRUE))\n\nstr(normalized_data)\n\n'data.frame':   26296 obs. of  14 variables:\n $ Gender                : Factor w/ 3 levels \"female\",\"male\",..: 2 1 3 1 2 1 3 1 2 1 ...\n $ Education_Level       : Ord.factor w/ 4 levels \"high school\"&lt;..: 2 3 1 2 1 3 1 1 1 1 ...\n $ Preferred_Activities  : Factor w/ 4 levels \"hiking\",\"skiing\",..: 2 2 1 1 1 2 2 1 2 2 ...\n $ Location              : Factor w/ 3 levels \"rural\",\"suburban\",..: 3 3 1 2 2 3 2 2 2 1 ...\n $ Favorite_Season       : Factor w/ 4 levels \"fall\",\"spring\",..: 3 4 3 3 1 1 3 2 2 2 ...\n $ Pets                  : int  0 0 1 1 0 1 0 1 0 1 ...\n $ Environmental_Concerns: int  1 0 0 1 0 1 1 0 0 0 ...\n $ Preference            : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Age                   : num  0.834 0.169 -0.763 -0.164 0.901 ...\n $ Income                : num  0.0525 -0.8116 1.0086 -0.5954 -0.1443 ...\n $ Travel_Frequency      : num  1.571 -1.561 0.527 0.875 -0.517 ...\n $ Vacation_Budget       : num  -0.202 -0.979 -0.969 -0.885 -0.102 ...\n $ Proximity_to_Mountains: num  0.4873 -0.7365 -1.2072 -0.0422 -1.1484 ...\n $ Proximity_to_Beaches  : num  1.19 1.34 1.04 1.03 1.08 ...\n\n\n\nnumeric_features = dplyr::select(normalized_data, where(is.numeric)) %&gt;% \n    select(where(~ n_distinct(.) &gt; 2))\n\nlong_data &lt;- numeric_features %&gt;%\n    pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"value\")\n\nggplot(long_data, aes(x = value)) +\n    geom_histogram(binwidth = 0.5, fill = \"steelblue\", color = \"white\") +\n    facet_wrap(~variable, scales = \"free\") +\n    labs(title = \"Histograms of Numeric Features\", x = \"Value\", y = \"Frequency\")"
  },
  {
    "objectID": "example_analysis.html#summary",
    "href": "example_analysis.html#summary",
    "title": "Example Analysis",
    "section": "Summary",
    "text": "Summary\nWe can see that our logistic regression was able to perfectly predict the preferences for our training data. This is pretty worthless because the error on the training data greatly underestimates the true error of our prediction function, and I did not save a test set because my goal was to identify which variables were the most important in influencing someone’s preference of mountains or beaches. Unfortunately, I can’t really accomplish that because none of the coefficients were statistically significant, which we can see in the summary of the coefficients. The P values are all greater than 0.5 (which is much too high). We can make some conclusions based on the size of the coefficients themselves, however. For example, large coefficients such as swimming and sunbathing as preferred activities make sense (-7.039e+03 and -7.038e+03). It also makes sense that factors such as education and gender are very insignificant predictors."
  },
  {
    "objectID": "example_analysis.html#packages-used",
    "href": "example_analysis.html#packages-used",
    "title": "Example Analysis",
    "section": "Packages Used",
    "text": "Packages Used\nDplyr: * select * where * across * mutate * n_distinct * everything\nggplot2: * geom_histogram * geom_bar * geom_line * geom_abline"
  },
  {
    "objectID": "example_analysis.html#data-dictionary",
    "href": "example_analysis.html#data-dictionary",
    "title": "Example Analysis",
    "section": "",
    "text": "data_dictionary = read.csv('data/data_dictionary.csv')\nknitr::kable(data_dictionary)\n\n\n\n\n\n\n\n\n\nvariables\ntype\ndescription\n\n\n\n\nAge\ninteger\nAge of the individual (numerical).\n\n\nGender\ncharacter\nGender identity of the individual (categorical: male, female, non-binary).\n\n\nIncome\ninteger\nAnnual income of the individual (numerical).\n\n\nEducation_Level\ncharacter\nHighest level of education attained (categorical: high school, bachelor, master, doctorate).\n\n\nTravel_Frequency\ninteger\nNumber of vacations taken per year (numerical).\n\n\nPreferred_Activities\ncharacter\nActivities preferred by individuals during vacations (categorical: hiking, swimming, skiing, sunbathing).\n\n\nVacation_Budget\ninteger\nBudget allocated for vacations (numerical).\n\n\nLocation\ncharacter\nType of residence (categorical: urban, suburban, rural).\n\n\nProximity_to_Mountains\ninteger\nDistance from the nearest mountains (numerical, in miles).\n\n\nProximity_to_Beaches\ninteger\nDistance from the nearest beaches (numerical, in miles).\n\n\nFavorite_Season\ncharacter\nPreferred season for vacations (categorical: summer, winter, spring, fall).\n\n\nPets\ninteger\nIndicates whether the individual owns pets (binary: 0 = No, 1 = Yes).\n\n\nEnvironmental_Concerns\ninteger\nIndicates whether the individual has environmental concerns (binary: 0 = No, 1 = Yes).\n\n\nPreference\ninteger\nIndicates whether the individual prefers mountains or beaches (binary: 0 = beaches, 1 = mountains)"
  },
  {
    "objectID": "example_analysis.html#logistic-regression",
    "href": "example_analysis.html#logistic-regression",
    "title": "Example Analysis",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nlog_reg = glm(formula = Preference ~ ., family = \"binomial\", data = normalized_data)\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(log_reg)\n\n\nCall:\nglm(formula = Preference ~ ., family = \"binomial\", data = normalized_data)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                     7.746e+02  1.646e+03   0.471    0.638\nGendermale                     -8.775e-01  8.182e+01  -0.011    0.991\nGendernon-binary               -4.294e-01  7.323e+01  -0.006    0.995\nEducation_Level.L               1.135e-01  6.589e+01   0.002    0.999\nEducation_Level.Q              -1.014e-01  5.822e+01  -0.002    0.999\nEducation_Level.C               7.034e-02  5.142e+01   0.001    0.999\nPreferred_Activitiesskiing     -2.353e-02  5.555e+01   0.000    1.000\nPreferred_Activitiessunbathing -7.011e+03  1.494e+04  -0.469    0.639\nPreferred_Activitiesswimming   -7.008e+03  1.493e+04  -0.469    0.639\nLocationsuburban               -4.504e-01  7.596e+01  -0.006    0.995\nLocationurban                  -9.393e-01  7.007e+01  -0.013    0.989\nFavorite_Seasonspring           2.647e-01  7.533e+01   0.004    0.997\nFavorite_Seasonsummer           4.340e-01  7.585e+01   0.006    0.995\nFavorite_Seasonwinter           3.475e-01  8.222e+01   0.004    0.997\nPets                            5.944e-01  5.696e+01   0.010    0.992\nEnvironmental_Concerns         -2.249e-01  5.628e+01  -0.004    0.997\nAge                            -3.849e-02  2.919e+01  -0.001    0.999\nIncome                         -1.653e-01  2.928e+01  -0.006    0.995\nTravel_Frequency               -1.075e-01  2.809e+01  -0.004    0.997\nVacation_Budget                -4.327e-02  2.863e+01  -0.002    0.999\nProximity_to_Mountains         -2.018e+03  4.282e+03  -0.471    0.637\nProximity_to_Beaches            2.009e+03  4.262e+03   0.471    0.637\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3.6454e+04  on 26295  degrees of freedom\nResidual deviance: 1.1741e-03  on 26274  degrees of freedom\nAIC: 44.001\n\nNumber of Fisher Scoring iterations: 25\n\n\nOh wow that’s pretty sad…I think I did not select the greatest dataset here. Nevertheless, we should plot some visuals.\n\nnormalized_data$predicted_prob &lt;- predict(log_reg, type = \"response\")\n\nggplot(normalized_data, aes(x = predicted_prob, fill = as.factor(Preference))) +\n    geom_histogram(binwidth = 0.05, position = \"identity\", alpha = 0.6) +\n    labs(title = \"Predicted Probability Distribution by Preference\",\n         x = \"Predicted Probability\", y = \"Count\", fill = \"Preference\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\nroc_obj &lt;- roc(normalized_data$Preference, normalized_data$predicted_prob)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nggplot(data = data.frame(tpr = roc_obj$sensitivities, fpr = 1 - roc_obj$specificities), aes(x = fpr, y = tpr)) +\n    geom_line(color = \"blue\") +\n    geom_abline(linetype = \"dashed\") +\n    labs(title = \"ROC Curve\", x = \"False Positive Rate\", y = \"True Positive Rate\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nthreshold &lt;- 0.5\n\nnormalized_data$predicted_class &lt;- ifelse(normalized_data$predicted_prob &gt; threshold, 1, 0)\n\nzero_one_loss &lt;- sum(normalized_data$predicted_class != normalized_data$Preference)\n\ncat('the zero one loss for our training set is:', zero_one_loss, '\\n')\n\nthe zero one loss for our training set is: 0"
  }
]